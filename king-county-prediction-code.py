# -*- coding: utf-8 -*-
"""statlearningproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ehUdo02yF1DanmpBbcBO747UVoLp5qQY
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import io

file_name = list(uploaded.keys())[0]
kc_house_data = pd.read_csv(io.BytesIO(uploaded[file_name]))

kc_house_data.head()

import seaborn as sns
import matplotlib.pyplot as plt

print("Dataset Overview:")
print(kc_house_data.info())

print("\nSummary Statistics:")
print(kc_house_data.describe())

print("\nMissing Values per Column:")
print(kc_house_data.isnull().sum())

import matplotlib.pyplot as plt

# Correlation Heatmap
plt.figure(figsize=(12, 8))
correlation_matrix = kc_house_data[['price', 'sqft_living', 'bedrooms', 'bathrooms', 'grade', 'sqft_above', 'sqft_basement', 'condition', 'lat', 'long', 'yr_built', 'yr_renovated', 'zipcode']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

# Pairplot of selected features
selected_features = ['price', 'sqft_living', 'bedrooms', 'bathrooms']
sns.pairplot(kc_house_data[selected_features])
plt.show()

# Distribution of house prices
plt.figure(figsize=(8, 5))
sns.histplot(kc_house_data['price'], kde=True, bins=30, color='blue')
plt.title("Distribution of House Prices")
plt.xlabel("Price")
plt.ylabel("Frequency")
plt.show()

"""Heatmap: Shows correlations between features like price, sqft_living, bedrooms, and others. For instance:

sqft_living has a strong positive correlation with price (0.7), meaning larger living spaces often sell for more.
grade is also positively correlated with price (0.67), suggesting better quality houses sell at higher prices.
Weak correlations, like between sqft_basement and price (0.32), indicate less influence on price.
This helps identify which features most influence house prices.

Pairplot: Displays relationships and distributions for price, sqft_living, bedrooms, and bathrooms:

Diagonals: Histograms show feature distributions. For example, price is right-skewed with many houses priced lower.
Scatterplots: Highlight trends, like sqft_living positively correlating with price (larger houses are more expensive), and bedrooms showing a weaker, more scattered relationship.
This gives insights into how key features interact with each other and with the target variable (price).

Histogram: Shows the distribution of house prices:

Most homes are priced below $1 million, with a peak in the lower ranges.
The right-skewed shape indicates a few high-value homes as outliers.
This helps understand the overall pricing landscape in the dataset.
"""

from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# Feature engineering
kc_house_data['price_per_sqft'] = kc_house_data['price'] / kc_house_data['sqft_living']
kc_house_data['bath_bed_ratio'] = kc_house_data['bathrooms'] / (kc_house_data['bedrooms'] + 1)  # Avoid division by zero
kc_house_data['total_rooms'] = kc_house_data['bedrooms'] + kc_house_data['bathrooms']

# Define features and target variable
features = ['sqft_living', 'grade', 'lat', 'long', 'bath_bed_ratio', 'total_rooms', 'condition', 'yr_built']
X = kc_house_data[features]
y = np.log1p(kc_house_data['price'])  # Log-transform the target to reduce skewness

# Impute missing values
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# XGBoost with hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'gamma': [0, 1],
    'tree_method': ['gpu_hist']  # Use GPU-accelerated tree growth
}

xgb = XGBRegressor(random_state=42, use_label_encoder=False, eval_metric='rmse')
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best model from grid search
best_xgb = grid_search.best_estimator_
y_pred_xgb = best_xgb.predict(X_test)

# Evaluate model
xgb_rmse = mean_squared_error(y_test, y_pred_xgb, squared=False)
xgb_r2 = r2_score(y_test, y_pred_xgb)

print("XGBoost (Tuned, GPU):")
print("RMSE:", np.expm1(xgb_rmse))  # Convert RMSE back to original scale
print("R2 Score:", xgb_r2)

from sklearn.linear_model import LinearRegression, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Linear Regression Model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

# Random Forest Model
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# Lasso Regression Model
lasso = Lasso(alpha=0.01, random_state=42)  # Adjust alpha to control regularization
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)

# Evaluation Metrics
def evaluate_model(name, y_test, y_pred):
    rmse = mean_squared_error(y_test, y_pred, squared=False)  # Root Mean Squared Error
    r2 = r2_score(y_test, y_pred)  # R-squared Score
    print(f"{name} Model:")
    print(f"  RMSE: {rmse}")
    print(f"  R2 Score: {r2}\n")

# Evaluate each model
evaluate_model("Linear Regression", y_test, y_pred_lr)
evaluate_model("Random Forest", y_test, y_pred_rf)
evaluate_model("Lasso Regression", y_test, y_pred_lasso)

from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Linear Regression Model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

# Random Forest Model
rf = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=20)  # Added hyperparameters
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# Lasso Regression Model
lasso = Lasso(alpha=0.01)  # Kept alpha for regularization
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)

# Evaluation Metrics
# Linear Regression
lr_rmse = mean_squared_error(y_test, y_pred_lr, squared=False)  # RMSE
lr_r2 = r2_score(y_test, y_pred_lr)  # R2 Score
print("Linear Regression:")
print("RMSE:", np.expm1(lr_rmse))  # Convert RMSE back to original scale
print("R2 Score:", lr_r2)

# Random Forest
rf_rmse = mean_squared_error(y_test, y_pred_rf, squared=False)  # RMSE
rf_r2 = r2_score(y_test, y_pred_rf)  # R2 Score
print("\nRandom Forest:")
print("RMSE:", np.expm1(rf_rmse))  # Convert RMSE back to original scale
print("R2 Score:", rf_r2)

# Lasso Regression
lasso_rmse = mean_squared_error(y_test, y_pred_lasso, squared=False)  # RMSE
lasso_r2 = r2_score(y_test, y_pred_lasso)  # R2 Score
print("\nLasso Regression:")
print("RMSE:", np.expm1(lasso_rmse))  # Convert RMSE back to original scale
print("R2 Score:", lasso_r2)

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Define the range of alpha values to test
param_grid = {'alpha': [0.01, 0.1, 0.5, 1, 10, 50, 100]}
lasso_cv = GridSearchCV(Lasso(random_state=42), param_grid, cv=5, scoring='r2')
lasso_cv.fit(X_train, y_train)

# Best alpha value
best_alpha = lasso_cv.best_params_['alpha']
print("Best alpha:", best_alpha)

# Train the Lasso model with the best alpha
lasso_optimized = Lasso(alpha=best_alpha, random_state=42)
lasso_optimized.fit(X_train, y_train)
y_pred_lasso_optimized = lasso_optimized.predict(X_test)

# Evaluation Metrics
lasso_rmse = mean_squared_error(y_test, y_pred_lasso_optimized, squared=False)  # RMSE
lasso_r2 = r2_score(y_test, y_pred_lasso_optimized)  # R2 Score

print("\nOptimized Lasso Regression:")
print("RMSE:", np.expm1(lasso_rmse))  # Convert RMSE back to the original scale
print("R2 Score:", lasso_r2)

from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np


#feature engineering
kc_house_data['bath_bed_ratio'] = kc_house_data['bathrooms'] / (kc_house_data['bedrooms'] + 1)  # Avoid division by zero
kc_house_data['appeal'] = kc_house_data['view'] + kc_house_data['waterfront']

#Unused features
kc_house_data['price_per_sqft'] = kc_house_data['price'] / kc_house_data['sqft_living']
kc_house_data['total_rooms'] = kc_house_data['bedrooms'] + kc_house_data['bathrooms']
kc_house_data['age'] = 2024 - kc_house_data['yr_built']
kc_house_data['home_features'] = kc_house_data['floors'] + kc_house_data['bathrooms'] + kc_house_data['bedrooms']

features = ['sqft_living', 'grade', 'bath_bed_ratio', 'appeal', 'lat', 'condition']
#features = ['sqft_living', 'grade', 'visibility', 'lat',  'condition', 'yr_renovated', 'home_features']
X = kc_house_data[features]
y = np.log1p(kc_house_data['price'])  # Log-transform the target to reduce skewness

# Impute missing values with mean
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Linear Regression Model
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
lr_rmse = mean_squared_error(y_test, y_pred_lr, squared=False)  # RMSE
lr_r2 = r2_score(y_test, y_pred_lr)  # R2 Score
print("Linear Regression:")
print("RMSE:", np.expm1(lr_rmse))  # Convert RMSE back to original scale
print("R2 Score:", lr_r2)

# Random Forest Model
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
rf_rmse = mean_squared_error(y_test, y_pred_rf, squared=False)  # RMSE
rf_r2 = r2_score(y_test, y_pred_rf)  # R2 Score
print("\nRandom Forest:")
print("RMSE:", np.expm1(rf_rmse))  # Convert RMSE back to original scale
print("R2 Score:", rf_r2)

# Polynomial Regression Model
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Fit a Linear Regression model on polynomial features
poly_model = LinearRegression()
poly_model.fit(X_train_poly, y_train)
y_pred_poly = poly_model.predict(X_test_poly)

# Evaluate the model
poly_r2 = r2_score(y_test, y_pred_poly)
poly_rmse = mean_squared_error(y_test, y_pred_poly, squared=False)  # RMSE
print("\nPolynomial Regression:")
print("RMSE:", np.expm1(poly_rmse))  # Convert RMSE back to original scale
print("R2 Score:", poly_r2)

# Lasso Regression
lasso = Lasso(alpha=0.01)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)
lasso_rmse = mean_squared_error(y_test, y_pred_lasso, squared=False)  # RMSE
lasso_r2 = r2_score(y_test, y_pred_lasso)  # R2 Score
print("\nLasso Regression:")
print("RMSE:", np.expm1(lasso_rmse))  # Convert RMSE back to original scale
print("R2 Score:", lasso_r2)

from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# XGBoost with hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'gamma': [0, 1],
    'tree_method': ['gpu_hist']  # Use GPU-accelerated tree growth
}

xgb = XGBRegressor(random_state=42, use_label_encoder=False, eval_metric='rmse')
grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best model from grid search
best_xgb = grid_search.best_estimator_
y_pred_xgb = best_xgb.predict(X_test)

# Evaluate model
xgb_rmse = mean_squared_error(y_test, y_pred_xgb, squared=False)
xgb_r2 = r2_score(y_test, y_pred_xgb)

print("XGBoost (Tuned, GPU):")
print("RMSE:", np.expm1(xgb_rmse))  # Convert RMSE back to original scale
print("R2 Score:", xgb_r2)

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Dimensionality Reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.8)
plt.title("PCA: King County Housing Data")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar(label='Log-Transformed Price')
plt.show()